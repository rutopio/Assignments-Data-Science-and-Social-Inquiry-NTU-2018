knitr::opts_chunk$set(echo = TRUE)
install.packages("jiebaR")
library(tidyverse)
library(stringr)
library(tidytext)
install.packages("tidytext")
install.packages("jiebaR")
library(tidyverse)
library(stringr)
library(tidytext)
library(jiebaR)
library(purrr)
library(lubridate)
library(scales)
knitr::opts_chunk$set(echo = TRUE)
install.packages("jiebaR")
library(tidyverse)
library(stringr)
library(tidytext)
library(jiebaR)
library(purrr)
library(lubridate)
library(scales)
news.df <- readRDS("data/typhoon.rds")
# 檢查一整個dataframe的數量，這裡顯示總共有4706個row，8個column。
dim(news.df)
# 檢查每一個column的標題
# 分別有link（連結）、title（標題）、time（時間）、report（寫報導的記者）、from（新聞源）、text（內文）、cat（時間區分，這裡的early是指1992年以前的新聞，late是指2016年後的新聞）
names(news.df)
# 因為中文不像英文那樣每個辭彙中間會空格，所以如果有辭彙不想被拆分的話就要窮舉出來
# 這裡把不想拆分開來的辭彙窮舉到segment_not裡面
segment_not <- c("質疑","中央","縣政府","市政府","中央政府","蘇南成","災前","災後","莫拉克","颱風","應變中心","停班停課","停課","停班","停駛","議會","路樹","阿扁","里長","賀伯","採收","菜價","蘇迪","受災戶","颱風警報","韋恩","台東縣","馬總統","馬英九","陳水扁","行政院長","立法院","陳總統","豪大雨","梅姬","台東","台北市政府","工務段","漂流木","陳菊","行政院","台南縣","卡玫基","魚塭","救助金","陳情","檢討","強颱","中颱","輕颱","小林村","野溪","蚵民","農委會","國民黨","民進黨","來襲","中油公司","中央政府","颱風天","土石流","蘇迪勒","水利署","陳說","颱風假","颱風地區","台灣","臺灣","柯羅莎","八八風災","紓困","傅崑萁","民進黨","國民黨","中央黨部","台中","鄉民","縣政府","表示","文旦柚","鄉鎮市公所","鄉鎮市","房屋稅","高雄","未達","台灣省","台北市","道歉","民調","八成","責任","政治責任")
# worker()函式是jiebaR這個library的東西，用來新建一個分詞引擎
# 這裡命名為cutter
# ref:https://zhuanlan.zhihu.com/p/25681782
cutter <- worker()
# new_user_word()同樣源自jiebaR，可以增加字定義的辭彙
# 換句話說，如果我們沒有加入segment_not，那麼jiebaR就會用預設的方法幫我們切詞
# ref:https://github.com/qinwf/jiebaR/issues/26
new_user_word(cutter, segment_not)
# 把讓切詞停止的keyword引入到stopWords，告訴jiebaR到這邊就要停下來
stopWords <- readRDS("data/stopWords.rds")
# 順便看一下裡面有啥
head(stopWords)
# 先切2000-2008年民進黨執政時期的資料
# tokenized.df就是我們等一下把東西切開來之後會存進去的dataframe
tokenized_DPP.df <- news.df %>%
# 後面的mutate是dplyr這個library特有的函式，用來新增dataframe的column
# 這裡在tokenized.df新增一個column，名為timestamp，把news.df裡的time轉存到這裡
mutate(timestamp=ymd(time)) %>%
# 如果我們要篩一段時間內的新聞就好，可以在這裡加上一個filter
# 這裡選2000-05-20到2008-05-20
filter(timestamp >= as.Date("2000-05-20") & timestamp < as.Date("2008-05-20")) %>%
# 因為我們有前面的timestamp了，這裡就不用news.df裡的time
# cat我們也用不到，把他移掉
select(-time,-cat,-report,-from) %>%
# 選取原本news.df裡面的這幾個column作為分析用
select(title, text, timestamp) %>%
# 在tokenized.df新增一個column，名為word，用來放切割後的詞
# 這段程式碼要拆成幾個部份，首先的purrr是一個讓R有類似OOP處理function的方法
# 因為我們要對每一個新聞的text部份進行切割，最簡單的方法就是寫一個for然後行他跑
# 可是這樣會有大量loop的部份，這時用map可以減少loop的使用，運行起來也比較快
# 他把原本的fumction包裝成另一個funciton以處理我們切割的執行
mutate(word = map(text, function(x)segment(x, cutter))) %>%
# 最後用party這個column標註這些是DPP時期的新聞
mutate(party='DPP')
# 先切2008-2016年國民黨執政時期的資料
tokenized_KMT.df <- news.df %>%
mutate(timestamp=ymd(time)) %>%
filter(timestamp >= as.Date("2008-05-20") & timestamp < as.Date("2016-05-20")) %>%
select(-time,-cat,-report,-from) %>%
select(title, text, timestamp) %>%
mutate(word = map(text, function(x)segment(x, cutter))) %>%
mutate(party='KMT')
# 打開幾篇新聞來檢查原本的新聞是否有被切開字了
tokenized_DPP.df$text[1]
tokenized_DPP.df$word[1]
tokenized_KMT.df$text[1]
tokenized_KMT.df$word[1]
# 最後合併成tokenized.df，再加上doc_id當index
tokenized.df <- rbind(tokenized_DPP.df,tokenized_KMT.df) %>% mutate(doc_id = row_number())
# 稍微看一下有什麼
head(tokenized.df)
# 這個動作會把上面的tokenized.df$word從array拆開成一個一個詞
unnested_DPP.df <- tokenized_DPP.df %>% unnest(word)
unnested_KMT.df <- tokenized_KMT.df %>% unnest(word)
unnested.df <- tokenized.df %>% unnest(word)
# 應該會有很多很多的row
dim(unnested_DPP.df)
dim(unnested_KMT.df)
dim(unnested.df)
knitr::opts_chunk$set(echo = TRUE)
install.packages("jiebaR")
library(tidyverse)
library(stringr)
library(tidytext)
library(jiebaR)
library(purrr)
library(lubridate)
library(scales)
news.df <- readRDS("data/typhoon.rds")
# 檢查一整個dataframe的數量，這裡顯示總共有4706個row，8個column。
dim(news.df)
# 檢查每一個column的標題
# 分別有link（連結）、title（標題）、time（時間）、report（寫報導的記者）、from（新聞源）、text（內文）、cat（時間區分，這裡的early是指1992年以前的新聞，late是指2016年後的新聞）
names(news.df)
# 因為中文不像英文那樣每個辭彙中間會空格，所以如果有辭彙不想被拆分的話就要窮舉出來
# 這裡把不想拆分開來的辭彙窮舉到segment_not裡面
segment_not <- c("質疑","中央","縣政府","市政府","中央政府","蘇南成","災前","災後","莫拉克","颱風","應變中心","停班停課","停課","停班","停駛","議會","路樹","阿扁","里長","賀伯","採收","菜價","蘇迪","受災戶","颱風警報","韋恩","台東縣","馬總統","馬英九","陳水扁","行政院長","立法院","陳總統","豪大雨","梅姬","台東","台北市政府","工務段","漂流木","陳菊","行政院","台南縣","卡玫基","魚塭","救助金","陳情","檢討","強颱","中颱","輕颱","小林村","野溪","蚵民","農委會","國民黨","民進黨","來襲","中油公司","中央政府","颱風天","土石流","蘇迪勒","水利署","陳說","颱風假","颱風地區","台灣","臺灣","柯羅莎","八八風災","紓困","傅崑萁","民進黨","國民黨","中央黨部","台中","鄉民","縣政府","表示","文旦柚","鄉鎮市公所","鄉鎮市","房屋稅","高雄","未達","台灣省","台北市","道歉","民調","八成","責任","政治責任")
# worker()函式是jiebaR這個library的東西，用來新建一個分詞引擎
# 這裡命名為cutter
# ref:https://zhuanlan.zhihu.com/p/25681782
cutter <- worker()
# new_user_word()同樣源自jiebaR，可以增加字定義的辭彙
# 換句話說，如果我們沒有加入segment_not，那麼jiebaR就會用預設的方法幫我們切詞
# ref:https://github.com/qinwf/jiebaR/issues/26
new_user_word(cutter, segment_not)
# 把讓切詞停止的keyword引入到stopWords，告訴jiebaR到這邊就要停下來
stopWords <- readRDS("data/stopWords.rds")
# 順便看一下裡面有啥
head(stopWords)
# 先切2000-2008年民進黨執政時期的資料
# tokenized.df就是我們等一下把東西切開來之後會存進去的dataframe
tokenized_DPP.df <- news.df %>%
# 後面的mutate是dplyr這個library特有的函式，用來新增dataframe的column
# 這裡在tokenized.df新增一個column，名為timestamp，把news.df裡的time轉存到這裡
mutate(timestamp=ymd(time)) %>%
# 如果我們要篩一段時間內的新聞就好，可以在這裡加上一個filter
# 這裡選2000-05-20到2008-05-20
filter(timestamp >= as.Date("2000-05-20") & timestamp < as.Date("2008-05-20")) %>%
# 因為我們有前面的timestamp了，這裡就不用news.df裡的time
# cat我們也用不到，把他移掉
select(-time,-cat,-report,-from) %>%
# 選取原本news.df裡面的這幾個column作為分析用
select(title, text, timestamp) %>%
# 在tokenized.df新增一個column，名為word，用來放切割後的詞
# 這段程式碼要拆成幾個部份，首先的purrr是一個讓R有類似OOP處理function的方法
# 因為我們要對每一個新聞的text部份進行切割，最簡單的方法就是寫一個for然後行他跑
# 可是這樣會有大量loop的部份，這時用map可以減少loop的使用，運行起來也比較快
# 他把原本的fumction包裝成另一個funciton以處理我們切割的執行
mutate(word = map(text, function(x)segment(x, cutter))) %>%
# 最後用party這個column標註這些是DPP時期的新聞
mutate(party='DPP')
# 先切2008-2016年國民黨執政時期的資料
tokenized_KMT.df <- news.df %>%
mutate(timestamp=ymd(time)) %>%
filter(timestamp >= as.Date("2008-05-20") & timestamp < as.Date("2016-05-20")) %>%
select(-time,-cat,-report,-from) %>%
select(title, text, timestamp) %>%
mutate(word = map(text, function(x)segment(x, cutter))) %>%
mutate(party='KMT')
# 打開幾篇新聞來檢查原本的新聞是否有被切開字了
tokenized_DPP.df$text[1]
tokenized_DPP.df$word[1]
tokenized_KMT.df$text[1]
tokenized_KMT.df$word[1]
# 最後合併成tokenized.df，再加上doc_id當index
tokenized.df <- rbind(tokenized_DPP.df,tokenized_KMT.df) %>% mutate(doc_id = row_number())
# 稍微看一下有什麼
head(tokenized.df)
# 這個動作會把上面的tokenized.df$word從array拆開成一個一個詞
unnested_DPP.df <- tokenized_DPP.df %>% unnest(word)
unnested_KMT.df <- tokenized_KMT.df %>% unnest(word)
unnested.df <- tokenized.df %>% unnest(word)
# 應該會有很多很多的row
dim(unnested_DPP.df)
dim(unnested_KMT.df)
dim(unnested.df)
# DPP的部份
unnested_DPP.df %>%
count(word, sort=T) %>%
# 這裡會篩掉stopWords裡的詞
filter(!(word %in% stopWords$word)) %>%
# 這裡會篩掉大小寫的單一英文字跟數字
filter(!str_detect(word, "[a-zA-Z0-9]+")) %>%
# 只篩出兩個字以上的詞，否則會篩出一堆「的」、「是」之類的東西
filter(nchar(word) > 1) %>%
# 只選出現三次的東西
filter(n > 3) %>%
# 重新排序
mutate(word = reorder(word, n)) %>%
# 只取前20名多的
slice(1:20) %>%
# 用ggplot畫圖的部份
ggplot() + aes(word, n)+
geom_col() +
ggtitle('2000-2008年民進黨執政時間聯合報最常用的字詞')+
ylab('出現次數')+
xlab('字詞')+
coord_flip() +
theme(axis.title.y=element_text(angle = 0,vjust = 0.5),plot.title = element_text(hjust = 0.5))
# KMT的部份
unnested_KMT.df %>%
count(word, sort=T) %>%
filter(!(word %in% stopWords$word)) %>%
filter(!str_detect(word, "[a-zA-Z0-9]+")) %>%
filter(nchar(word) > 1) %>%
filter(n > 3) %>%
mutate(word = reorder(word, n)) %>%
# 這裡取20筆就好
slice(1:20) %>%
ggplot() + aes(word, n)+
ggtitle('2008-2016年國民黨執政時間聯合報最常用的字詞')+
ylab('出現次數')+
xlab('字詞')+
geom_col() +
coord_flip() +
theme(axis.title.y=element_text(angle = 0,vjust = 0.5),plot.title = element_text(hjust = 0.5))
# 統計用詞出現的次數
party_word.tf <- unnested.df %>%
# 記數
count(party, word) %>%
# 一樣篩掉不想統計的東西
filter(!str_detect(word, "[a-zA-Z0-9]+")) %>%
filter(!(word %in% stopWords$word)) %>%
filter(nchar(word)>1)
# 統計比例
party_ratio <- party_word.tf %>%
filter(n>1) %>%
spread(party, n, fill = 0) %>%
ungroup() %>%
# 計算比例，最後取log
mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %>%
mutate(logratio = log(KMT / DPP)) %>%
arrange(desc(logratio))
# 畫圖
party_ratio %>%
group_by(logratio > 0) %>%
top_n(10, abs(logratio)) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
geom_bar(stat = "identity") +
coord_flip() +
ggtitle('聯合報社於兩黨執政時期，用詞出現的次數頻率之比')+
xlab('字詞')+
ylab("兩黨執政時期用詞出現的次數頻率之比（經取natural log後處理）") +
scale_fill_manual(name = "", labels = c("KMT","DPP"),values = c("tomato","lightblue")) +
theme(axis.title.y=element_text(angle = 0,vjust = 0.5),plot.title = element_text(hjust = 0.5))
# 分析用字的頻率，愈靠近兩軸代表愈靠近兩黨
frequency <- party_word.tf %>%
# 一樣只篩出現次數大於三次的字詞
filter(n>3) %>%
group_by(party) %>%
mutate(proportion = n/sum(n)) %>%
select(-n) %>%
spread(party, proportion) %>%
na.omit()
# 畫圖
frequency %>%
ggplot(aes(x = DPP, y = KMT, color = abs(KMT - DPP))) +
geom_abline(color = "gray40", lty = 5) +
geom_point(alpha = 0.1, size = 2.0, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, size = 3) +
theme(legend.position="none") +
# 調整成取log_10的狀態，免得資料太過集中
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
coord_fixed(1)+
ggtitle('聯合報社於兩黨執政時期，用詞出現的次數頻率散佈圖')+
xlab('2000-2008民進黨執政期間')+
ylab('2008-2016國民黨執政期間')+
theme(axis.title.y=element_text(angle = 0,vjust = 0.5),plot.title = element_text(hjust = 0.5))
setwd("~/文件/DSSI")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(httr)
library(dplyr)
options(stringsAsFactors = F)
url <- "https://dq.yam.com/list.php"
doc   <- read_html(url)
# 就剛剛說的h3標籤
titlepath <- '//h3'
# 發佈時間和內文都算好找
timepath <- '//figcaption/time'
contentpath <- '//figcaption/p'
# 新聞觀看人數就藏在比較裡面的地方
viewpath <- '//ul/li[1]/span/text()'
# 下面開始讀，並用length()函數檢查這幾個有沒有都抓到相等的數量
title_ <- html_text(html_nodes(doc, xpath = titlepath))
length(title_)
time_ <- html_text(html_nodes(doc, xpath = timepath))
length(time_)
content_ <- html_text(html_nodes(doc, xpath = contentpath))
length(content_)
view_ <- html_text(html_nodes(doc, xpath = viewpath))
length(view_)
# 現在開始抓連結，因為我已知第一個連結是廣告，所以我本來的想法是抓
# //*[@id="pixList"]/div[@class='imgWrap masonry-brick']/a/@href
# 但我怎麼抓都抓不到...只好繞遠路抓全部的連結，再把第一個連結刪掉
linkpath <- '//*[@id="pixList"]/div/a'
link_ <- html_attr(html_nodes(doc, xpath = linkpath), "href")
# 刪掉第一個廣告連結，留下有用的
link_ = link_[2:length(link_)]
# 補上前面，在檢查，應該要跟上面的大家一樣多個
link_ <- paste0("https://dq.yam.com/", link_)
length(link_)
# 好ㄉ我們抓完第一頁啦，先暫時把定存成dataframe這樣比較好看
tmp <- cbind(title_, content_, time_, view_, link_)
tmp <- as.data.frame(tmp)
head(tmp)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(httr)
library(dplyr)
options(stringsAsFactors = F)
url <- "https://dq.yam.com/list.php"
doc   <- read_html(url)
# 就剛剛說的h3標籤
titlepath <- '//h3'
# 發佈時間和內文都算好找
timepath <- '//figcaption/time'
contentpath <- '//figcaption/p'
# 新聞觀看人數就藏在比較裡面的地方
viewpath <- '//ul/li[1]/span/text()'
# 下面開始讀，並用length()函數檢查這幾個有沒有都抓到相等的數量
title_ <- html_text(html_nodes(doc, xpath = titlepath))
length(title_)
time_ <- html_text(html_nodes(doc, xpath = timepath))
length(time_)
content_ <- html_text(html_nodes(doc, xpath = contentpath))
length(content_)
view_ <- html_text(html_nodes(doc, xpath = viewpath))
length(view_)
# 現在開始抓連結，因為我已知第一個連結是廣告，所以我本來的想法是抓
# //*[@id="pixList"]/div[@class='imgWrap masonry-brick']/a/@href
# 但我怎麼抓都抓不到...只好繞遠路抓全部的連結，再把第一個連結刪掉
linkpath <- '//*[@id="pixList"]/div/a'
link_ <- html_attr(html_nodes(doc, xpath = linkpath), "href")
# 刪掉第一個廣告連結，留下有用的
link_ = link_[2:length(link_)]
# 補上前面，在檢查，應該要跟上面的大家一樣多個
link_ <- paste0("https://dq.yam.com/", link_)
length(link_)
# 好ㄉ我們抓完第一頁啦，先暫時把定存成dataframe這樣比較好看
tmp <- cbind(title_, content_, time_, view_, link_)
tmp <- as.data.frame(tmp)
head(tmp)
#前輟
pre <- "https://dq.yam.com/list.php?page="
# 指定要抓幾頁，就先抓十頁吧
page <- 10
# 先清空我們最後要放的東西
all_title_ <- c()
all_time_ <- c()
all_content_ <- c()
all_view_ <- c()
all_link_ <- c()
# 迴圈
for(p in 1:page){
url <- paste0(pre,p)
doc <- read_html(url)
# 跟上面一樣
titlepath <- '//h3'
timepath <- '//figcaption/time'
contentpath <- '//figcaption/p'
viewpath <- '//ul/li[1]/span/text()'
# 每次都把抓到的東西和已經有的東西（all開頭的vector）合併
title_ <- html_text(html_nodes(doc, xpath = titlepath))
all_title_ <- c(all_title_,title_)
time_ <- html_text(html_nodes(doc, xpath = timepath))
all_time_ <- c(all_time_,time_)
content_ <- html_text(html_nodes(doc, xpath = contentpath))
all_content_ <- c(all_content_,content_)
view_ <- html_text(html_nodes(doc, xpath = viewpath))
all_view_ <- c(all_view_,view_)
linkpath <- '//*[@id="pixList"]/div/a'
link_ <- html_attr(html_nodes(doc, xpath = linkpath), "href")
link_ = link_[2:length(link_)]
link_ <- paste0("https://dq.yam.com/", link_)
all_link_ <- c(all_link_,link_)
}
# 最後大合併成df
df <- cbind(all_title_,all_content_,all_time_,all_view_,all_link_)
df <- as.data.frame(df)
# 加上df的Column名稱
colnames(df) <- c('新聞標題','新聞摘要','發佈時間','觀看人數','新聞連結')
# 檢查是否抓到每頁16篇*10頁=共160篇新聞
nrow(df)
#欣賞一下
head(df)
View(df)
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(jsonlite)
library(dplyr)
# 先保留抓下來str還是str型態
options(stringsAsFactors = FALSE)
# 設定編碼
options(encoding = "UTF-8")
# 就是我們剛在小秘密裡面的開頭
dcardurl <- 'https://www.dcard.tw/_api/forums/'
# 設定我們要抓的看板
board <- 'relationship'
# 融合成一個網址，在這裡我們將抓的順序設定成照時間排序而非照熱門排序，所以用false
mainurl <- paste0(dcardurl,board,'/posts?popular=false')
# 抽出json，把他存入resdata這個data.frame裡面
resdata <- fromJSON(content(GET(mainurl), "text"))
# 看看前幾筆的Dcard幻想文，檢查我們是否有抓到東西
# 因為東西實在是太多了，我們就先看前兩個Column就好，下面亦同
head(resdata[,1:2])
# 還記得在上面分析小秘密的時候，我們提到before=id代表他讀取到這篇文章之前的文章
# 所以為了讓我們繼續往下抓，我們要知道我們目前抓到的最後一篇文章id，然後把他設為before=id，才能繼續抓更早的文章
tail(resdata[,1:2])
# 把最後一篇文章的id設為變數end
end <- resdata$id[length(resdata$id)]
# 看一下是否就是最後一筆
end
# 假設我們要抓150篇文章
n <- 150
# 因為我們不去改limit的值，所以他預設會每次抓30篇回來，我們把要抓的文章/30便是我們要抓的次數
# 還要再減一，因為我們一開始就先抓了前30筆
# 也可以不館前面我們抓了什麼，從頭開始抓就不用減一，但我比較喜歡這麼寫
page <- (150/30)-1
# 寫一個loop，重複做page次
for(i in 1:page){
# 從「目前抓到的最後一篇文章id」往前抓30篇
url <- paste0(mainurl,"&before=",end)
# 測試時可以把url印出來檢查有沒有抓對
# print(url)
# 把抓到存入暫存的tmpres，這只是暫存
tmpres <- fromJSON(content(GET(url), "text"))
# 從tmpres裡更新「最後一篇文章的id」
end <- tmpres$id[length(tmpres$id)]
# 然後把我們新抓到的tmpres和之前已經有的resdata合併
resdata <- bind_rows(resdata,tmpres)
}
# 要不要做都可以，但做了可以省記憶體
rm(tmpres)
# 看看前幾筆
head(resdata[,1:2])
# 看看最後的幾筆
tail(resdata[,1:2])
# 看看總共有幾筆
nrow(resdata)
